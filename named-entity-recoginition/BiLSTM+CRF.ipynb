{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BiLSTM+CRF.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEaZ1JENBfZ64y/ZvzrIKK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-nAIUzJAo8R","executionInfo":{"status":"ok","timestamp":1621251410340,"user_tz":-480,"elapsed":20713,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"ccdd5a82-24bb-42b7-adf9-9291d4cf19ec"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MF0ry1_aBGQj","executionInfo":{"status":"ok","timestamp":1621251416608,"user_tz":-480,"elapsed":729,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"047523cb-c58d-48eb-8816-f14549b3bdd5"},"source":["!ls /content/drive/MyDrive/NaturalLanguageProcessing/named-entity-recoginition"],"execution_count":2,"outputs":[{"output_type":"stream","text":["BiLSTM+CRF.ipynb  data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Tn5S2LGBVyL","executionInfo":{"status":"ok","timestamp":1621251420397,"user_tz":-480,"elapsed":2610,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}}},"source":["import pandas as pd\n","import numpy as np\n","np.random.seed(1)\n","\n","import matplotlib.pyplot as plt\n","\n","from itertools import chain\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","tf.random.set_seed(2)\n","\n","from tensorflow.keras import Sequential, Model, Input\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"cW7EL8B2BW9Y","executionInfo":{"status":"ok","timestamp":1621251424156,"user_tz":-480,"elapsed":1685,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"568dbd63-cf01-4362-978d-56cc946aa6be"},"source":["FILE_PATH=\"/content/drive/MyDrive/NaturalLanguageProcessing/named-entity-recoginition/data/ner_dataset.csv\"\n","\n","data = pd.read_csv(FILE_PATH, encoding= 'unicode_escape')\n","data.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1          NaN             of   IN   O\n","2          NaN  demonstrators  NNS   O\n","3          NaN           have  VBP   O\n","4          NaN        marched  VBN   O"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"HRQ_71BnB-H4","executionInfo":{"status":"ok","timestamp":1621251427088,"user_tz":-480,"elapsed":739,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}}},"source":["def get_dict_map(data, token_or_tag):\n","    tok2idx = {}\n","    idx2tok = {}\n","    \n","    if token_or_tag == 'token':\n","        vocab = list(set(data['Word'].to_list()))\n","    else:\n","        vocab = list(set(data['Tag'].to_list()))\n","    \n","    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n","    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n","    return tok2idx, idx2tok\n","\n","\n","token2idx, idx2token = get_dict_map(data, 'token')\n","tag2idx, idx2tag = get_dict_map(data, 'tag')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"w6X1w_qcCbR9","executionInfo":{"status":"ok","timestamp":1621251429408,"user_tz":-480,"elapsed":729,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"cbaf8b04-4d72-4b6a-f62b-0f80d07e89f5"},"source":["data['Word_idx'] = data['Word'].map(token2idx)\n","data['Tag_idx'] = data['Tag'].map(tag2idx)\n","data.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>Word_idx</th>\n","      <th>Tag_idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>16523</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>7192</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>24687</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>30875</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>23163</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n","0  Sentence: 1      Thousands  NNS   O     16523       12\n","1          NaN             of   IN   O      7192       12\n","2          NaN  demonstrators  NNS   O     24687       12\n","3          NaN           have  VBP   O     30875       12\n","4          NaN        marched  VBN   O     23163       12"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"id":"Wze4_PFaCjM6","executionInfo":{"status":"ok","timestamp":1621251438273,"user_tz":-480,"elapsed":7144,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"e9cfcc98-4a88-49b2-8418-bf7c771b39a8"},"source":["data_fillna = data.fillna(method='ffill', axis=0)\n","# Groupby and collect columns\n","data_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n","# Visualise data\n","data_group.head()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>Word_idx</th>\n","      <th>Tag_idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n","      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n","      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n","      <td>[16523, 7192, 24687, 30875, 23163, 5997, 20624...</td>\n","      <td>[12, 12, 12, 12, 12, 12, 8, 12, 12, 12, 12, 12...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 10</td>\n","      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n","      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n","      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n","      <td>[29018, 9245, 32710, 34542, 33634, 16926, 9291...</td>\n","      <td>[5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 100</td>\n","      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n","      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n","      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n","      <td>[15927, 11949, 6599, 11912, 21661, 28937, 3230...</td>\n","      <td>[12, 12, 0, 12, 12, 12, 12, 12, 8, 12, 12, 12,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1000</td>\n","      <td>[They, left, after, a, tense, hour-long, stand...</td>\n","      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n","      <td>[1638, 8449, 26898, 34602, 35143, 30855, 17361...</td>\n","      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 10000</td>\n","      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n","      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n","      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n","      <td>[27271, 27112, 21553, 33384, 21136, 15711, 224...</td>\n","      <td>[8, 12, 12, 10, 15, 12, 0, 12, 8, 12, 5, 12, 5...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Sentence #  ...                                            Tag_idx\n","0      Sentence: 1  ...  [12, 12, 12, 12, 12, 12, 8, 12, 12, 12, 12, 12...\n","1     Sentence: 10  ...  [5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...\n","2    Sentence: 100  ...  [12, 12, 0, 12, 12, 12, 12, 12, 8, 12, 12, 12,...\n","3   Sentence: 1000  ...       [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n","4  Sentence: 10000  ...  [8, 12, 12, 10, 15, 12, 0, 12, 8, 12, 5, 12, 5...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uES3CUWNDJwQ","executionInfo":{"status":"ok","timestamp":1621251445119,"user_tz":-480,"elapsed":1853,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"35432932-e205-4fdb-ff56-199540ba0844"},"source":["def get_pad_train_test_val(data_group, data):\n","\n","    #get max token and tag length\n","    n_token = len(list(set(data['Word'].to_list())))\n","    n_tag = len(list(set(data['Tag'].to_list())))\n","\n","    #Pad tokens (X var)    \n","    tokens = data_group['Word_idx'].tolist()\n","    maxlen = max([len(s) for s in tokens])\n","    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n","\n","    #Pad Tags (y var) and convert it into one hot encoding\n","    tags = data_group['Tag_idx'].tolist()\n","    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n","    n_tags = len(tag2idx)\n","    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n","    \n","    #Split train, test and validation set\n","    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n","    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n","\n","    print(\n","        'train_tokens length:', len(train_tokens),\n","        '\\ntrain_tokens length:', len(train_tokens),\n","        '\\ntest_tokens length:', len(test_tokens),\n","        '\\ntest_tags:', len(test_tags),\n","        '\\nval_tokens:', len(val_tokens),\n","        '\\nval_tags:', len(val_tags),\n","    )\n","    \n","    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n","\n","train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["train_tokens length: 32372 \n","train_tokens length: 32372 \n","test_tokens length: 4796 \n","test_tags: 4796 \n","val_tokens: 10791 \n","val_tags: 10791\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLj4oZaZEFXT","executionInfo":{"status":"ok","timestamp":1621251448315,"user_tz":-480,"elapsed":743,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"8bc8999a-c807-480f-f2a0-6f2ea2f14075"},"source":["input_dim = len(list(set(data['Word'].to_list())))+1 # add new tag\n","output_dim = 64\n","input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n","n_tags = len(tag2idx)\n","print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["input_dim:  35179 \n","output_dim:  64 \n","input_length:  104 \n","n_tags:  17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGjczTuiEKAA","executionInfo":{"status":"ok","timestamp":1621251450921,"user_tz":-480,"elapsed":716,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}}},"source":["def get_bilstm_lstm_model():\n","    model = Sequential()\n","\n","    # Add Embedding layer\n","    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n","\n","    # Add bidirectional LSTM\n","    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n","\n","    # Add LSTM\n","    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n","\n","    # Add timeDistributed Layer\n","    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n","\n","    #Optimiser \n","    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n","\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","    \n","    return model"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQKfINJ1EO61","executionInfo":{"status":"ok","timestamp":1621251454037,"user_tz":-480,"elapsed":1194,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}}},"source":["def train_model(X, y, model):\n","    loss = list()\n","    for i in range(5):\n","        # fit model for one epoch on this sequence\n","        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n","        loss.append(hist.history['loss'][0])\n","    return loss"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngUFFWJFEVLX","executionInfo":{"status":"ok","timestamp":1621251617875,"user_tz":-480,"elapsed":161936,"user":{"displayName":"Xiaoyang Xiong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgLMYZJarpanJRTQmqh4C0xPPlps4Kwv13xIFkAlQ=s64","userId":"04998332129346917011"}},"outputId":"60bb3c6c-f52a-4e0c-9427-31deb4040119"},"source":["results = pd.DataFrame()\n","model_bilstm_lstm = get_bilstm_lstm_model()\n","plot_model(model_bilstm_lstm)\n","results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 104, 64)           2251456   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, 104, 128)          66048     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 104, 64)           49408     \n","_________________________________________________________________\n","time_distributed (TimeDistri (None, 104, 17)           1105      \n","=================================================================\n","Total params: 2,368,017\n","Trainable params: 2,368,017\n","Non-trainable params: 0\n","_________________________________________________________________\n","26/26 [==============================] - 38s 1s/step - loss: 1.4292 - accuracy: 0.8265 - val_loss: 0.4700 - val_accuracy: 0.9681\n","26/26 [==============================] - 29s 1s/step - loss: 0.4626 - accuracy: 0.9674 - val_loss: 0.3489 - val_accuracy: 0.9681\n","26/26 [==============================] - 29s 1s/step - loss: 0.3188 - accuracy: 0.9677 - val_loss: 0.2705 - val_accuracy: 0.9681\n","26/26 [==============================] - 29s 1s/step - loss: 0.2770 - accuracy: 0.9677 - val_loss: 0.2585 - val_accuracy: 0.9681\n","26/26 [==============================] - 29s 1s/step - loss: 0.2655 - accuracy: 0.9677 - val_loss: 0.2475 - val_accuracy: 0.9681\n"],"name":"stdout"}]}]}